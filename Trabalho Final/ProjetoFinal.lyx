#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\renewcommand \thefigure{Figura~\@arabic\c@figure}
\def\fps@figure{tbp}
\def\ftype@figure{1}
\def\ext@figure{lof}
\def\fnum@figure{\thefigure}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language brazilian
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing other 1.5
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 3cm
\rightmargin 3cm
\bottommargin 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center

\size large
FUNDAÇÃO GETULIO VARGAS
\end_layout

\begin_layout Standard
\align center

\size large
ESCOLA DE MATEMÁTICA APLICADA
\end_layout

\begin_layout Standard
\align center

\size large
MESTRADO 2015.3
\end_layout

\begin_layout Standard
\align center
APRENDIZAGEM POR MÁQUINAS
\end_layout

\begin_layout Standard
\align center

\size large
Prof Eduardo Mendes
\end_layout

\begin_layout Standard
\begin_inset VSpace 4cm
\end_inset


\end_layout

\begin_layout Standard
\align center

\size large
Projeto Final
\end_layout

\begin_layout Standard
\begin_inset VSpace 4cm
\end_inset


\end_layout

\begin_layout Standard
\align center
KIZZY TERRA
\end_layout

\begin_layout Standard
\align center
LUCAS MACHADO
\end_layout

\begin_layout Standard
\begin_inset VSpace 5cm
\end_inset


\end_layout

\begin_layout Standard
\align center
RIO DE JANEIRO
\end_layout

\begin_layout Standard
\align center
DEZEMBRO/2015
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introdução
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Este relatório visa descrever o processo de implementação das soluções submetida
s na Competição de Reconhecimento de Dígitos do Kaggle (https://www.kaggle.com/c/d
igit-recognizer).
 O desempenho destas soluçõeos nesta competição será utilizado na disciplina
 de Aprendizado de Máquina, correspondendo ao seu projeto final, a fim de
 avaliar a habilidade dos alunos de utilizar técnicas de machine learning
 para solucionar um problema real.
 
\end_layout

\begin_layout Section
A Competição de Reconhecimento de Dígitos
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
O objetivo da competição é reconhecer dígitos em imagens que contém dígitos
 escritos a mão.
 Os dados para a compentição foram retirados do MNIST dataset.
 O MNIST (”Modified National Institute of Standards and Technology”) dataset
 é um conjunto de dados clássico dentro da comunidade de Aprendizado de
 Máquina e tem sido largamente estudado (mais detalhes em http://yann.lecun.com/ex
db/mnist/index.html).
\end_layout

\begin_layout Section
Pré-processamento dos dados
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
O pré-processamento dos dados é a primeira etapa (desconsiderando o processo
 de obtenção) e é importante para conseguir aprender eficientemente o comportame
nto dos dados.
 A remoção de dimensões pouco relevantes na aproximação da 
\emph on
target function
\emph default
, a centralização, que é uma transformação que translada os dados de entrada
 para a origem removendo viés, e a normalização dos dados, que garante que
 todas as dimensões estão na mesma escala, são exemplos de pré-processamentos
 que são importantes de ser aplicados sobre diversos conjuntos de dados
 antes de utilizá-los em um modelo.
\end_layout

\begin_layout Standard
No contexto do problema de reconhecimento de dígitos, cada pixel da imagem
 é considerado uma dimensão.
 Com isso, uma imagem 28x28 pixels tem 784 dimensões, e a correlação entre
 essas dimensões é alta.
 Para tratar os dados, foram utilizadas quatro técnicas de pré-processamento:
 centralização, normalização, 
\emph on
data whitening 
\emph default
e 
\emph on
principal component analysis (PCA)
\emph default
.
\end_layout

\begin_layout Standard
A centralização é uma transformação simples e geralmente benigna que translada
 os dados para a origem removendo qualquer viés dos dados.
\end_layout

\begin_layout Standard
A normalização garante que todas as dimensões estão na mesma escala, o que
 é uma transformação importante para muitos modelos, como por exemplo os
 que utilizam a distância euclidiana.
 Uma motivação a mais que tivemos para normalizar os dados foi a utilização
 de 
\emph on
support vector machines (SVM)
\emph default
 com o 
\emph on
radius basis function kernel.

\emph default
 O 
\emph on
rbf kernel
\emph default
 é dado por 
\begin_inset Formula $K(x,y)=exp(-c*||x-y||^{2})$
\end_inset

.
 Sendo assim, se o dominío das dimensões for muito largo (0-255), então
 
\begin_inset Formula $||x-y||^{2}$
\end_inset

 será muito grande, comprometendo a eficiência do método.
\end_layout

\begin_layout Standard
O 
\emph on
data
\emph default
 whitening
\emph on
 
\emph default
é utilizado para duas principais finalidades: diminuir a correlação entre
 as features existentes e forçar todas as 
\emph on
features 
\emph default
a terem a mesma variância.
 Esta método é bastante utilizado no contexto de classificação de imagens
 devido a alta redundância de informação presente em pixels adjacentes.
 No caso de imagens, pixels próximos fornecem informações altamente correlaciona
das e o objetivo do 
\emph on
whitening 
\emph default
é diminuir a quantidade de informação redundante.
\end_layout

\begin_layout Standard
Por último, a análise de componentes principais, o qual tem como finalidade
 selecionar as dimensões que possuem a maiori influência no processo de
 generalização.
 Se uma dimensão possui o valor 0 na maioria das imagens, ele não possui
 pouca informação sobre o conjunto de dados.
\end_layout

\begin_layout Standard
A análise de componentes principais foi feita utilizando-se a classe 
\emph on
decomposition
\emph default
 do pacote 
\emph on
Scikit Learn.
 
\emph default
Uma vez feita a decomposição, foi possível gerar um gráfico com a relação
 entre fração de variância e o número de componentes geradas.
 A informação obtida com este gráfico acarretou a conlusão de que utilizar
 de 35 a 40 componentes poderia levar a um resultado satisfatório.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename variance-features.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Variância x Número de 
\emph on
features
\emph default
 geradas
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Seleção de Modelos
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A seleção de modelos adequados a serem utilizados na classificação de um
 determinado conjunto de dados deve considerar os tipos de dados, a quantidade
 de dados disponível, a complexidade da classificação, a acurácia na classificaç
ão e a velocidade de classificação.
 
\end_layout

\begin_layout Standard
No contexto de uma competição do Kaggle, a variável de maior prioridade
 é a acurácia na classificação uma vez que submete-se o resultado da classificaç
ão sobre o conjunto de teste e não o algoritmo implementado em si.
 
\end_layout

\begin_layout Standard
Evidentemente, a acurácia de um classificador baseia-se no conjunto de dados
 que está sendo analisado, consequentemente esta escolha exige experimentação
 e por essa razão a etapa de seleção de modelos consiste de elencar alguns
 modelos candidatos cuja performance deverá ser comparada para a classificação
 do conjunto de dados do MNIST.
 A escolha destes candidatos por sua vez, leva em conta as características
 e vantagens dos principais modelos conhecidos.
 A descrição dos classificadores escolhidos se encontram na seção 5.
\end_layout

\begin_layout Section
Classificadores Utilizados
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Nesta seção descreve-se os modelos selecionados, os quais foram utilizados
 para a classificação do conjunto de dados do MNIST.
 A descrição destes algoritmos compreende uma breve análise teórica - incluindo
 comentários sobre a seleção de parâmetros - e uma análise da respectiva
 performance obtida.
 
\end_layout

\begin_layout Subsection
Regressão Logística
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
O primeiro modelo utilizado para classificação foi um modelo de classificação
 linear: regressão logística.
 A regressão logística pode ser vista como uma generalização da regressão
 linear para os problemas de classificação de variáveis categóricas.
 Uma grande diferença entre os dois modelos está no fato de que a regressão
 logística prevê a probabilidade de resultados particulares atarvés da função
 de distribuição de logística.
 Os valores previstos neste de classificação são, portanto pertencentes
 ao intervalo 
\begin_inset Formula $(0,1)$
\end_inset

.
 Desta forma, a regressão logística prevê a probabilidade de resultados
 particulares.
 
\end_layout

\begin_layout Standard
A implementação do modelo de Regressão Logística utilizada foi a do pacote
 
\emph on
sklearn.linear_model 
\emph default
feito para a linguagem 
\emph on
Python.

\emph default
 A performance obtida com este classificador na submissão no Kaggle foi
 de 
\begin_inset Formula $91,00\%$
\end_inset


\end_layout

\begin_layout Subsection
k-NN
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
O k-NN té uma generalização do algoritmo 
\emph on
Nearest Neighbors 
\emph default
o qual consiste de considerar a informação dos pontos vizinhos mais próximos
 como uma aproximação para um determindado ponto que se quer classificar.
 Este é um algoritmo simples cuja vantagem é a não exigência de tempo de
 treinamento, entretanto os requisitos de memória e tempo de classificação
 são relativamente grandes.
\end_layout

\begin_layout Standard
O parâmetro k controla o trade off entre a aproximação e a generalização.
 Escolhendo
\begin_inset Formula $k$
\end_inset

 muito pequeno obtém-se uma regra de decisão complexa, o que causa overfitting
 nos dados; Por outro lado, um 
\begin_inset Formula $k$
\end_inset

 muito garnde conduz a underfitting.
\end_layout

\begin_layout Standard
Para a o problema de classificação dos dígitos optou-se por utilizar o modelo
 com 
\begin_inset Formula $k=3$
\end_inset

 e a acurácia obtida na submissão foi de 
\begin_inset Formula $97.70\%$
\end_inset

.
\end_layout

\begin_layout Subsection
SVM e 
\emph on
kernel
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Dentre os modelos utilizados, o que apresentou o melhor resultado foi 
\emph on
support vector machines
\emph default
 (SVM) com 
\emph on
kernel
\emph default
.
 Uma grande vantagem deste modelo é sua robustez a ruído, ou seja, é mais
 difícil de ocorrer 
\emph on
overfitting
\emph default
.
 O 
\emph on
kernel
\emph default
 se constitui de método eficiente para utilizar transformações não-lineares
 em grandes dimensões.
 Ou seja, temos um modelo não-linear com regularização automática.
\end_layout

\begin_layout Standard
Um modelo SVM é uma representação dos exemplos presentes no conjunto de
 treinamento como pontos no espaço, mapeados de maneira que os exemplos
 de cada classe sejam divididos por hiperplanos ótimos.
\end_layout

\begin_layout Standard
Dado o pré-processamento descrito na seção 3, foram utilizados 40 componentes
 a partir do PCA, e o kernel utilizado foi o 
\emph on
radius basis function.
 
\emph default
Com isso, dois hiperparâmetros precisamos ser escolhidos: o parâmetro de
 penalização C e parâmetro 
\emph on
gamma
\emph default
, que especifica a 
\begin_inset Quotes eld
\end_inset

largura
\begin_inset Quotes erd
\end_inset

 do kernel.
\end_layout

\begin_layout Standard
Para a escolha de tais parâmetros foi utilizado o método 
\emph on
cross-validation 
\emph default
com 
\emph on
kfold=3.
 
\emph default
O melhor par de parâmetros foi escolhido dentre 15 valores de C gerados
 aleatoriamentro entre
\emph on
 
\begin_inset Formula $10^{-2}$
\end_inset

 
\emph default
e
\emph on
 
\begin_inset Formula $10^{4}$
\end_inset

 
\emph default
e 15 valores para gamma entre 
\begin_inset Formula $10^{-4}$
\end_inset

e 
\begin_inset Formula $10^{3}$
\end_inset

.
\end_layout

\begin_layout Standard
Após 4 horas o algoritmo retornou como melhores hiperparâmetros 
\begin_inset Formula $C=2.276$
\end_inset

 e 
\begin_inset Formula $gamma=0$
\end_inset

.
 O erro 
\emph on
cross-validation
\emph default
 foi 
\begin_inset Formula $0.01700$
\end_inset

 e, após a submissão no 
\emph on
kaggle
\emph default
, o erro 
\emph on
out-of-sample
\emph default
 foi 
\begin_inset Formula $0.01571$
\end_inset

.
\end_layout

\begin_layout Standard
Este foi o modelo de melhor performance e, consequentemente, o modelo escolhido
 para a solução do desafio.
\end_layout

\begin_layout Section
Conclusão
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
indent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
O desenvolvimento do Projeto Final proposto permitiu que fossem colocados
 os aprendizados adquiridos ao longo da disciplina em prática, tendo sido,
 por esta razão, uma experiência bastante proveitosa e interessante.
\end_layout

\begin_layout Standard
De certo modo, o modelo de projeto escolhido - uma competição do Kaggle
 - é um motivador a mais quando se está buscando boas soluções que permitam
 obter a melhor performance possível na solução do desafio.
\end_layout

\end_body
\end_document
